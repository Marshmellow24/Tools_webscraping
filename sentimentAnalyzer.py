from langdetect import detect
from textblob_de import TextBlobDE as TextBlob
from textblob import TextBlob as TextBlobEN
from langdetect import detect, detect_langs
from json_parser import *
import re


def clean_file(file):
    """
    This method cleans the files which ideally contain one comment per line. It is mainly for cleaning textual data
    generated by the IGJson_Parser. If the comments are split by their post shortcodes and delimiter these will be
    removed first

    :param file: text file
    :return: list of strings
    """
    with open(file, encoding="utf-8") as f:
        data = f.readlines()

    # regex patterns for data cleaning of json parsed files by IGJsonParser with delimiter and number options

    remove_linesep = re.compile(r"\n", re.IGNORECASE)
    # TODO: this removes every token with 11 characters and no space. should only remove shortcodes though
    remove_id = re.compile(r"^.{11}$", re.IGNORECASE)
    remove_asterisks = re.compile(r"^\*+", re.IGNORECASE)
    remove_index = re.compile(r"^\d+$", re.IGNORECASE)

    output = []
    for sentence in data:
        linesep_removed = re.sub(remove_linesep, "", sentence)
        id_removed = re.sub(remove_id, "", linesep_removed)
        ast_removed = re.sub(remove_asterisks, "", id_removed)
        index_removed = re.sub(remove_index, "", ast_removed)

        if index_removed:  # only add non-empty strings to list
            output.append(index_removed)

    return output


# allocate sentences to lists according to their detected language

def allocate_lang(data, lang):
    """
    Detects german language and others too, sort german tokens into a list and rest of tokens into another list

    :param data: list of strings
    :param lang: list of specified language that shall be put out. if lang ="" it gives out both lists as tuple
    :return: specified list of tokens, sorted by detected language
    """
    cleaned_de = []
    cleaned_en = []
    cleaned_langs = (cleaned_de, cleaned_en)

    for clean_token in data:
        error = None
        try:
            detect(clean_token)
        except Exception as e:
            error = e
        finally:
            if error is not None:
                continue
            elif detect(clean_token) != "de":
                cleaned_en.append(clean_token.strip())
            else:
                cleaned_de.append(clean_token.strip())

    if lang == "de":
        return cleaned_de
    elif lang == "en":
        return cleaned_en
    else:
        return cleaned_langs


# create list of tuples containing cleaned tokens and sentiment probabilities

def compute_sentiments(data):
    """
    Calculates the sentiment values for each token and appends them to list

    :param data: list of tokens
    :return: list of tuples containing tokens with sentiment probabilities
    """

    sentiments = []
    for token in data:
        blob = TextBlob(token)
        sentiments.append((token, blob.sentiment.subjectivity))
    for token in data:
        blob = TextBlobEN(token)
        sentiments.append((token, blob.sentiment.subjectivity))
    return sentiments


# merge all functions into this function

def sentiments_token_combo(filepath, lang="de"):
    output = compute_sentiments(allocate_lang(clean_file(filepath), lang))
    return output


input_file = r"\directory\subdirectory\file.txt"  # example, insert file path here

# example, here: compute sentiments for english tokens for file above and print them in one column
print(*sentiments_token_combo(input_file, "en"), sep='\n')

